# GPU vs CPU Performance Analysis for Financial Risk Calculations

This notebook analyzes the performance differences between CPU and GPU implementations for financial risk calculations, specifically focusing on covariance matrix computation.

## 1. Import Libraries


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
import sys
import os

sys.path.append('../src')
from cpu_implementation import run_cpu_analysis
from gpu_implementation import run_gpu_analysis
```

## 2. Define Performance Testing Functions


```python
def run_performance_test(file_path):
    """
    Run both CPU and GPU implementations and compare performance
    
    Parameters:
    file_path (str): Path to the stock price data file
    
    Returns:
    dict: Performance metrics and results from both implementations
    """
    print(f"Running performance test for {file_path}...")
    
    # Run CPU implementation
    print("Running CPU implementation...")
    cpu_results = run_cpu_analysis(file_path)
    
    # Run GPU implementation
    print("Running GPU implementation...")
    gpu_results = run_gpu_analysis(file_path)
    
    # Calculate speedup
    cpu_cov_time = cpu_results["covariance_time"]
    gpu_cov_time = gpu_results["covariance_time"]
    cov_speedup = cpu_cov_time / gpu_cov_time if gpu_cov_time > 0 else 0
    
    cpu_total_time = cpu_results["total_execution_time"]
    gpu_total_time = gpu_results["total_execution_time"]
    total_speedup = cpu_total_time / gpu_total_time if gpu_total_time > 0 else 0
    
    # Check result consistency
    cpu_cov_matrix = cpu_results["covariance_matrix"]
    gpu_cov_matrix = gpu_results["covariance_matrix"]
    
    # Calculate mean absolute difference between CPU and GPU results
    mean_abs_diff = np.abs(cpu_cov_matrix - gpu_cov_matrix).mean().mean()
    
    return {
        "cpu_cov_time": cpu_cov_time,
        "gpu_cov_time": gpu_cov_time,
        "cpu_total_time": cpu_total_time,
        "gpu_total_time": gpu_total_time,
        "cov_speedup": cov_speedup,
        "total_speedup": total_speedup,
        "mean_abs_diff": mean_abs_diff,
        "data_size": os.path.basename(file_path).split("_")[2].split(".")[0],
        "cpu_cov_matrix": cpu_cov_matrix,
        "gpu_cov_matrix": gpu_cov_matrix
    }
```

## 3. Run Performance Tests


```python
# List of dataset files
datasets = [
    "../data/stock_prices_10000_days.csv",
    "../data/stock_prices_100000_days.csv",
    "../data/stock_prices_1000000_days.csv"
]

# Run tests and collect results
results = []
for dataset in datasets:
    result = run_performance_test(dataset)
    results.append(result)
    
# Convert results to DataFrame for easier analysis
results_df = pd.DataFrame([
    {
        "Dataset Size": result["data_size"],
        "CPU Covariance Time (s)": result["cpu_cov_time"],
        "GPU Covariance Time (s)": result["gpu_cov_time"],
        "CPU Total Time (s)": result["cpu_total_time"],
        "GPU Total Time (s)": result["gpu_total_time"],
        "Covariance Speedup": result["cov_speedup"],
        "Total Speedup": result["total_speedup"],
        "Result Difference": result["mean_abs_diff"]
    }
    for result in results
])

results_df
```

## 4. Visualize Performance Comparison


```python
# Set up the visualization style
plt.style.use('ggplot')
sns.set_palette("Set2")
plt.figure(figsize=(14, 8))

# Create a bar chart comparing CPU vs GPU execution times
x = np.arange(len(results_df["Dataset Size"]))
width = 0.35

fig, ax = plt.subplots(figsize=(12, 6))
cpu_bars = ax.bar(x - width/2, results_df["CPU Total Time (s)"], width, label='CPU')
gpu_bars = ax.bar(x + width/2, results_df["GPU Total Time (s)"], width, label='GPU')

# Add labels, title and legend
ax.set_xlabel('Dataset Size (Days)', fontsize=12)
ax.set_ylabel('Execution Time (seconds)', fontsize=12)
ax.set_title('CPU vs GPU Total Execution Time by Dataset Size', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(results_df["Dataset Size"])
ax.legend()

# Add execution time values on top of each bar
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.2f}s',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(cpu_bars)
add_labels(gpu_bars)

plt.tight_layout()
plt.savefig('../results/visualizations/cpu_vs_gpu_chart.png', dpi=300)
plt.show()
```

## 5. Analyze Covariance Matrix Computation Speedup


```python
# Create a line chart showing speedup trends as dataset size increases
plt.figure(figsize=(12, 6))
plt.plot(results_df["Dataset Size"], results_df["Covariance Speedup"], 'o-', linewidth=2, markersize=8)
plt.axhline(y=1.0, color='r', linestyle='--', alpha=0.7, label='Break-even point')

plt.xlabel('Dataset Size (Days)', fontsize=12)
plt.ylabel('Speedup Factor (CPU Time / GPU Time)', fontsize=12)
plt.title('GPU Acceleration Factor for Covariance Matrix Calculation', fontsize=14)
plt.grid(True, alpha=0.3)
plt.legend()

# Annotate each point with the speedup value
for i, speedup in enumerate(results_df["Covariance Speedup"]):
    plt.annotate(f'{speedup:.2f}x', 
                 xy=(results_df["Dataset Size"].iloc[i], speedup),
                 xytext=(5, 5),
                 textcoords='offset points')

plt.tight_layout()
plt.savefig('../results/visualizations/speedup_trend.png', dpi=300)
plt.show()
```

## 6. Compare Result Accuracy


```python
# For the first dataset, compare some values from CPU and GPU covariance matrices
sample_result = results[0]
cpu_matrix = sample_result["cpu_cov_matrix"]
gpu_matrix = sample_result["gpu_cov_matrix"]

# Display a portion of both matrices and their difference
comparison_df = pd.DataFrame({
    'CPU Value': cpu_matrix.values.flatten()[:10],
    'GPU Value': gpu_matrix.values.flatten()[:10],
    'Absolute Difference': np.abs(cpu_matrix.values.flatten() - gpu_matrix.values.flatten())[:10]
})

comparison_df
```

## 7. Summary and Conclusions


```python
# Create a summary of findings
summary_data = {
    'Metric': [
        'Best CPU Performance (s)',
        'Best GPU Performance (s)',
        'Maximum Speedup Factor',
        'Dataset Size for Max Speedup',
        'Average Result Difference'
    ],
    'Value': [
        results_df["CPU Total Time (s)"].min(),
        results_df["GPU Total Time (s)"].min(),
        results_df["Covariance Speedup"].max(),
        results_df.loc[results_df["Covariance Speedup"].idxmax(), "Dataset Size"],
        results_df["Result Difference"].mean()
    ]
}

summary_df = pd.DataFrame(summary_data)
summary_df
```

### Key Findings:

1. The GPU implementation shows significant performance advantages for large datasets, with the most pronounced speedup observed for the 1,000,000-day dataset.

2. For smaller datasets (10,000 days), the CPU actually outperforms the GPU slightly, likely due to the overhead of data transfer between CPU and GPU memory.

3. The speedup factor increases with dataset size, indicating that GPU parallelization benefits scale with larger workloads.

4. The numerical results from both implementations are extremely close, with only minimal differences that can be attributed to floating-point precision variations.

5. The total execution time, which includes data loading, daily return calculations, and VaR calculations, shows that GPU acceleration provides an overall performance advantage for financial risk analysis pipelines.

These findings demonstrate that GPU acceleration is particularly valuable for large-scale financial risk analysis, while traditional CPU implementations remain efficient for smaller analytical tasks.
